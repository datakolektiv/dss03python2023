{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67b65bdc",
   "metadata": {},
   "source": [
    "# DATA SCIENCE SESSIONS VOL. 3\n",
    "### A Foundational Python Data Science Course\n",
    "## Tasklist 12: Statistical Hypothesis Testing + Correlation(s).\n",
    "\n",
    "[&larr; Back to course webpage](https://datakolektiv.com/)\n",
    "\n",
    "Feedback should be send to [goran.milovanovic@datakolektiv.com](mailto:goran.milovanovic@datakolektiv.com). \n",
    "\n",
    "These notebooks accompany the DATA SCIENCE SESSIONS VOL. 3 :: A Foundational Python Data Science Course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4dbdf7",
   "metadata": {},
   "source": [
    "![](../img/IntroRDataScience_NonTech-1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e97431",
   "metadata": {},
   "source": [
    "### Lecturers\n",
    "\n",
    "[Goran S. Milovanović, PhD, DataKolektiv, Chief Scientist & Owner](https://www.linkedin.com/in/gmilovanovic/)\n",
    "\n",
    "[Aleksandar Cvetković, PhD, DataKolektiv, Consultant](https://www.linkedin.com/in/alegzndr/)\n",
    "\n",
    "[Ilija Lazarević, MA, DataKolektiv, Consultant](https://www.linkedin.com/in/ilijalazarevic/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9da7dd",
   "metadata": {},
   "source": [
    "![](../img/DK_Logo_100.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52c2322",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc6f6683",
   "metadata": {},
   "source": [
    "### Intro \n",
    "\n",
    "The goal of this Tasklist is to consolidate our knowledge of applied Probability Theory so to be able to formulate and solve problems in the Statistical Hypothesis Framework that we have covered in our Session 12. Later on we will touch upon correlation(s) again. Let's do it: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5b40a79",
   "metadata": {},
   "source": [
    "Import the Python modules that you think you will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84ef5bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6523ac68",
   "metadata": {},
   "source": [
    "**N.B.** We have some datasets prepared for this task list, found in the `_data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "613bb18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chi_square_table.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "work_dir = os.getcwd()\n",
    "data_dir = os.path.join(work_dir, \"_data\")\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af1584f6",
   "metadata": {},
   "source": [
    "**01.** We will use the `chi_square_table.csv` dataset here.Read through the secion on $\\chi^2$ distrubution and the related $\\chi^2$ statistical test in Session 12 again. Using only Numpy, Pandas, and SciPy, perform a $\\chi^2$ test for the following data set (you will need to compute he test stastics by hand - use Numpy). Compute the probability of Type I Error (the $p$ value), report the test statistic, the $p$ value, and make a conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "356a64b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c05ceef7",
   "metadata": {},
   "source": [
    "**02.** Use the same dataset as in **01.** to do the following:\n",
    "\n",
    "- study the [scipy.stats.chisquare](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chisquare.html) documentation\n",
    "- import `chisquare` from `scipy.stats`\n",
    "- and use the `chisquare` method to perform the $\\chi^2$ test.\n",
    "- Note: `ddof` in `scipy.stats.chisquare` is your number of degrees of freedom, which is $k-1$ for the $\\chi^2$ test with $k$ categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb112d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7633198",
   "metadata": {},
   "source": [
    "Confused by something in the results? Well, that is the reason why perhaps you should learn how to do Statistical Hypothesis Testing by hand! Read through this discussion (but do not expect the answer to what worries you): [Getting nan for p values in scipy chisquare: Don't know why?](https://stackoverflow.com/questions/40380217/getting-nan-for-p-values-in-scipy-chisquare-dont-know-why)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b614c8b4",
   "metadata": {},
   "source": [
    "**03.** A t-test against a constant value: create a random vector of size `1,000` from the Normal Distribution with $\\mu=5$ and $\\sigma=2$ and test if its mean is statistically significantly different from `5.3`. Use SciPy to compute the t-test and report your results (you will probably need to import something). Explain the finding in your own words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aacb71dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd429bf8",
   "metadata": {},
   "source": [
    "**04.** A t-test for independent measures. We are assuming the following:\n",
    "\n",
    "- People educated in Math via method A score, on the average, 78 points on some standardized test X; while\n",
    "- people educated in Math via method B score, on the average, 65 points on some standardized test X;\n",
    "- the standard deviation of the test scores is always 14.\n",
    "\n",
    "Generate two random vectors of size `1,000` each that represent this hypotheses and perform an independent measures t-test for two group means using SciPy. Explain: what justifies the use of and *independent t-test* in this case? Are the two methods empirically really likely to yield different results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94fea61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bde66c58",
   "metadata": {},
   "source": [
    "**05.** A t-test for repeated measures (paired t-test). We are assuming the following:\n",
    "\n",
    "- Before training with DataKolektiv, people typically (on the average) can score 67 points on some standardize test Y;\n",
    "- while after training with DataKolektiv they score 88 points on the average;\n",
    "- it is known that the standard deviation of the test scores is 28.\n",
    "\n",
    "Generate two random vectors of size `1,000` each that represent these hypotheses and perform an dependent measures t-test for two group means using SciPy. Explain: what justifies the use of a *paired t-test* in this case? According to the test result, is the DataKolektiv training really effective? Explain the finding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a386bca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c827d180",
   "metadata": {},
   "source": [
    "**06.** Correlation. Generate two random, normally distributed vectors with $\\mu=5$ and $\\mu=7.5$, both having the same $\\sigma=2.3$, both of size `1,000`. Use Numpy `np.cov()` (**N.B.** mind the documentation - the `ddof` argument!) to compute the sample covariance between these two vectors, then standardize them and compute the Pearson correlation coefficient. Use Pandas to visualize produce the respective scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55dd403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc0348",
   "metadata": {},
   "source": [
    "DataKolektiv, 2022/23.\n",
    "\n",
    "[hello@datakolektiv.com](mailto:goran.milovanovic@datakolektiv.com)\n",
    "\n",
    "![](../img/DK_Logo_100.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8e160a",
   "metadata": {},
   "source": [
    "<font size=1>License: [GPLv3](https://www.gnu.org/licenses/gpl-3.0.txt) This Notebook is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This Notebook is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this Notebook. If not, see http://www.gnu.org/licenses/.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
