{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4609c518",
   "metadata": {},
   "source": [
    "# DATA SCIENCE SESSIONS VOL. 3\n",
    "### A Foundational Python Data Science Course\n",
    "## Session 16: Generalized Linear Models I. Binomial Logistic Regression and its MLE. Multinomial Regression\n",
    "\n",
    "[&larr; Back to course webpage](https://datakolektiv.com/)\n",
    "\n",
    "Feedback should be send to [goran.milovanovic@datakolektiv.com](mailto:goran.milovanovic@datakolektiv.com). \n",
    "\n",
    "These notebooks accompany the DATA SCIENCE SESSIONS VOL. 3 :: A Foundational Python Data Science Course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd35403",
   "metadata": {},
   "source": [
    "![](../img/IntroRDataScience_NonTech-1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cca64d",
   "metadata": {},
   "source": [
    "### Lecturers\n",
    "\n",
    "[Goran S. Milovanović, PhD, DataKolektiv, Chief Scientist & Owner](https://www.linkedin.com/in/gmilovanovic/)\n",
    "\n",
    "[Aleksandar Cvetković, PhD, DataKolektiv, Consultant](https://www.linkedin.com/in/alegzndr/)\n",
    "\n",
    "[Ilija Lazarević, MA, DataKolektiv, Consultant](https://www.linkedin.com/in/ilijalazarevic/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3963c0a8",
   "metadata": {},
   "source": [
    "![](../img/DK_Logo_100.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef0626",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03d98ebe",
   "metadata": {},
   "source": [
    "## **Generalized Linear Models**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9dd5ec8",
   "metadata": {},
   "source": [
    "We will now begin to introduce a set of extremely useful statistical learning models. Their name - *Generalized Linear Models (GLMs)* - suggests that their are somehow related to Simple and Multiple Linear Regression models and yet somehow go beyond them. That is correct: GLMs generalize the linear model, where predictors and their respective coefficients produce a linear combination of vectors, by introducing *link* functions to solve those kinds of problems that cannot be handled by Linear Regression. For example, what if the problem is not to predict a continuous value of the criterion, but the outcome variable is rather a dichotomy and then the problem becomes the one of categorization? E.g. predict the sex of a respondent given a set ${X}$ of their features? Enters *Binomial Logistic Regression*, the simplest GLM. \n",
    "\n",
    "Another thing: GLMs cannot be estimated by minimizing the quadratic error as we have estimated Simple and Multiple Linear Regression in the previous Session15. The method used to estimate Linear Models is known as *Least Squares Estimation*. To fit GLMs to our data, we will introduce the concept of *Likelihood* in Probability Theory and learn about the *Maximum Likelihood Estimate*!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22127b7d",
   "metadata": {},
   "source": [
    "### What happens when the assumptions of the Linear Model fail?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ce1ec30",
   "metadata": {},
   "source": [
    "Let us briefly recall the assumptions of the (Multiple) Linear Regression model:\n",
    "\n",
    "+ *Variables are real numbers*: both outcome and predictor variables are members of $R$, the set of real numbers; at least in theory they can take any real value from `-Inf` to `Inf`.\n",
    "+ *Linearity*: there must be a linear relationship between outcome variable and the predictor variable(s).\n",
    "+ *Normality*: it is assumed that the residuals (i.e model errors) are normally distributed.\n",
    "+ *Homoscedasticity*: the variances of error terms (i.e. residuals) are similar across the values of the predictor variables.\n",
    "+ *No autocorrelation*: the residuals are not autocorrelated.\n",
    "+ *No influential cases*: no outliers are present.\n",
    "+ *No Multicollinearity* (in Multiple Regression only): the predictors are not that highly correlated with each other.\n",
    "\n",
    "What if we observe a set of variables that somehow describe a statistical experiment that can result in any of the two discrete outcomes? For example, we observe a description of a behavior of a person, quantified in some way, and organized into a set of variables that should be used to predict the sex of that person? Or any other similar problem where the outcome can take only two values, say `0` or `1` (and immediately recall the Binomial Distribution)?\n",
    "\n",
    "The assumptions of the Linear Model obviously constrain its application in such cases. We ask the following question now: would it be possible to *generalize*, or *expand*, *modify* the Linear Model somehow to be able to encompass the categorization problem? Because it sounds so appealing to be able to have a set of predictors, combine them in a linear fashion, and estimate the coefficients so to be able to predict whether the outcome would turn this way or another?\n",
    "\n",
    "There is a way to develop such a generalization of the Linear Model. In its simplest form it represents the *Binomial Logistic Regression*. Binomial Logistic Regression is very similar to multiple regression, except that for the outcome variable is now a *categorical variable* (i.e. it is measured on a nominal scale that is a *dichotomy*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4bd6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Setup - importing the libraries\n",
    "\n",
    "# - supress those annoying 'Future Warning'\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# - data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# - os\n",
    "import os\n",
    "\n",
    "# - ml\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# - visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# - parameters\n",
    "%matplotlib inline\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "sns.set_theme()\n",
    "# - rng\n",
    "rng = np.random.default_rng()\n",
    "# - plots\n",
    "plt.rc(\"figure\", figsize=(8, 6))\n",
    "plt.rc(\"font\", size=14)\n",
    "sns.set_theme(style='white')\n",
    "\n",
    "# - directory tree\n",
    "data_dir = os.path.join(os.getcwd(), '_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6eeb8a",
   "metadata": {},
   "source": [
    "## 1. Binomial Logistic Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc0ce4ec",
   "metadata": {},
   "source": [
    "Let's recall the form of the Linear Model with any number of predictors:\n",
    "\n",
    "$$Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_kX_k + \\epsilon$$\n",
    "\n",
    "So we have a linear combination of $k$ predictors $\\boldsymbol{X}$ plus the model error term $\\epsilon$ on the RHS, and the outcome variable $Y$ on the LHS. \n",
    "\n",
    "Now we assume that $Y$ can take only two possible values, call them $0$ and $1$ for ease of discussion. We want to predict whether $Y$ will happen to be ($1$) or not ($0$) given our observations of a set of predictors $\\boldsymbol{X}$. However, in Binary Logistic Regression we do not predict the value of the outcome itself, but rather the *probability* that the outcome will turn out $1$ or $0$ given the predictors. \n",
    "\n",
    "In the simplest possible case, where there is only one predictor $X_1$, this is exactly what we predict in Binary Logistic Regression:\n",
    "\n",
    "$$P(Y) = p_1 =  \\frac{1}{1+e^{-(\\beta_0 + \\beta_1X_1)}}$$\n",
    "where $\\beta_0$ and $\\beta_1$ are the same old good linear model coefficients. As we will see, the linear coefficients have a new interpretation in Binary Logistic Regression - a one rather different that the one they receive in the scope of the Linear Model.\n",
    "\n",
    "With $k$ predictors we have:\n",
    "\n",
    "$$P(Y) = p_1 = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_kX_k)}}$$\n",
    "Now the above equations looks like it felt from the clear blue sky to solve the problem. There is a clear motivation for its form, of course: imagine that instead of predicting the state of $Y$ directly we decide to predicts the *odds* of $Y$ turning out $1$ instead of $0$:\n",
    "\n",
    "$$odds = \\frac{p_1}{1-p_1}$$\n",
    "Now goes the trick: if instead of predicting the odds $p_1/(1-p_1)$ we decide to predict the **log-odds** (also called: *logit*) from a linear combination of predictors\n",
    "\n",
    "$$log \\left( \\frac{p_1}{1-p_1} \\right) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_kX_k$$\n",
    "it turns out that we can recover the odds by taking the *exponent* of both LHS and RHS:\n",
    "\n",
    "$$\\frac{p_1}{1-p_1} = e^{(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_kX_k)}$$\n",
    "and then by following a simple algebraic rearrangement:\n",
    "\n",
    "(1) let's write $l=\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_kX_k$ for simplicity and use $l$ to replace the whole linear combination in whats follows;\n",
    "\n",
    "(2) we have $log \\left( \\frac{p_1}{1-p_1} \\right)=l$ then;\n",
    "\n",
    "(3) taking the exponent of both sides we arive at $\\frac{p_1}{1-p_1}=e^l$;\n",
    "\n",
    "(4) immediately follows that\n",
    "\n",
    "$\\frac{p_1}{1-p_1}=\\frac{1}{e^{-l}} \\implies p_1=\\frac{1-p_1}{e^{-l}} \\implies p_1+\\frac{p1}{e^{-l}}=\\frac{1}{e^{-l}}\\implies p_1e^{-l}+p_1=1 \\implies p_1(1+e^{-l})=1$\n",
    "\n",
    "and after rewriting $l$ as a linear combination again we find that the probability $p_1$ of the outcome $Y$ turning out $1$ is:\n",
    "\n",
    "$$P(Y) = p_1 = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_kX_k)}}$$\n",
    "\n",
    "Now, imagine we set a following criterion: anytime we estimate $p_1$ to be larger than or equal to $.5$ we predict that $Y=1$, and anytime $p_1 < .5$ we predict that $Y=0$. What we need to do in order to be able to learn how to predict $Y$ in this way is to estimate the coefficients $b_0$, $b_1$, $b_2$, etc like we did in the case of a linear model. However, minimizing SSE will not work in this case: our predictions will be on a probability scale, while our observations are discrete, $0$ or $1$. We will have to find another way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaf645f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - loading the dataset\n",
    "# - GitHub: https://github.com/dijendersaini/Customer-Churn-Model/blob/master/churn_data.csv\n",
    "# - place it in your _data/ directory\n",
    "churn_data = pd.read_csv(os.path.join(data_dir, 'churn_data.csv'))\n",
    "churn_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced58821",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1701d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd136c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - some entries have missing values given as empty strings\n",
    "churn_data.loc[488]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2d3928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - use .replace method to replace empty strings with NaN values\n",
    "churn_data = churn_data.replace(r'^\\s*$', np.nan, regex=True)\n",
    "churn_data.loc[488]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5c655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - we drop all the entries with missing values\n",
    "churn_data = churn_data.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8488f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642e39fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - notice that 'TotalCharges' values are non-numeric type, but they should be\n",
    "# - this is due to the empty string values that were previously present\n",
    "# - we convert them to numeric type\n",
    "churn_data['TotalCharges'] = churn_data['TotalCharges'].astype('float')\n",
    "churn_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b1a4ce",
   "metadata": {},
   "source": [
    "### Target: Predict churn from all numeric predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3000126",
   "metadata": {},
   "source": [
    "We use Binomial Logistic Regression to predict the probability $p$ of a given observation $\\textbf{x}$, with features $(x_1, x_2, \\ldots, x_k)$, belonging to one of the two binary categories \\{0, 1\\}. We compute these probabilites via\n",
    "\n",
    "$$p = \\frac{1}{1+e^{\\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_kx_k + \\beta_0}},$$\n",
    "\n",
    "where $\\beta_1, \\beta_2, \\ldots, \\beta_k$ are the model's parameters for the predictors, and $\\beta_0$ is the intercept of the model.\n",
    "\n",
    "In order to determine whether the predicted label $\\hat{y}$ for a given observation $\\textbf{x}$ has binary label 1 or 0, we impose a decision criterion $\\sigma$ - a number in the (0, 1) interval. If $p > \\sigma$, then we assign label $\\hat{y} = 1$ to the observation $\\textbf{x}$; else, we take $\\hat{y} = 0$. Ususally, we take $\\sigma = 0.5$.\n",
    "\n",
    "The model is optimized by MLE (Maximum Likelihood Estimation), and the interpretation of the model coefficients is the following:\n",
    "\n",
    "- for a given predictor $x_i$, the exponential of its coefficient, $e^{\\beta_i}$ tells us about the change $\\Delta_{odds}$, where $\\Delta_{odds}$ is the difference between $\\frac{p_1}{1-p_1}$ *following* a unit increase in $x_i$ and before it - given that everything else is kept constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d28531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Preparing the model frame\n",
    "\n",
    "# - extracting 'Churn' and all the numerical features columns\n",
    "model_frame = churn_data[['Churn', 'tenure', 'MonthlyCharges', 'TotalCharges']]\n",
    "model_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c330493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - encoding 'Churn' values to binary values\n",
    "model_frame['Churn'] = model_frame['Churn'].apply(lambda x: int(x == 'Yes'))\n",
    "model_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e35668",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = model_frame.columns.drop('Churn')\n",
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7368bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Composing the fomula of the model\n",
    "\n",
    "# - right side of the formula\n",
    "formula = ' + '.join(predictors)\n",
    "\n",
    "# - left side of the formula\n",
    "formula = 'Churn ~ ' + formula\n",
    "\n",
    "formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b5c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - fitting BLR model to the data\n",
    "binomial_linear_model = smf.logit(formula=formula, data=model_frame).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bad920",
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial_linear_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24ab1bdb",
   "metadata": {},
   "source": [
    "**N.B.** There is a bug in the Wald's Z, look:\n",
    "\n",
    "> The reason why the Wald statistic should be used cautiously is because, when the regression coefficient is large, the standard error tends to become inflated, resulting in the Wald statistic being underestimated (see Menard, 1995). The inflation of the standard error increases the probability of rejecting a predictor as being significant when in reality it is making a significant\n",
    "contribution to the model (i.e. you are more likely to make a Type II error). From: Andy Field, DISCOVERING STATISTICS USING SPSS, Third Edition, Sage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3770b3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - model's parameters\n",
    "binomial_linear_model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5334dc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - exponential of the model's parameters\n",
    "np.exp(binomial_linear_model.params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40517544",
   "metadata": {},
   "source": [
    "### Coefficients in BLR\n",
    "\n",
    "The $\\Delta Odds$ (Odds Ratio)\n",
    "\n",
    "Do not forget that you have transformed your linear combination of model coefficients and predictors into a log-odds space: the logistic regression coefficient $\\beta$ associated with a predictor X is the expected change in **log(odds)**.\n",
    "\n",
    "So, by taking $e^{\\beta_i}$, your coefficient now says:\n",
    "\n",
    "- take the odds **after** a unit change in the predictor $X_i$\n",
    "- take the **original odds** (before the unit change in predictor $X_i$)\n",
    "- $\\Delta Odds$ = (odds after a unit change in the predictor)/(original odds)\n",
    "- will change by $e^{\\beta_i}$.\n",
    "\n",
    "Which means that \n",
    "- if $e^{\\beta_i}>1$, then predictor $X_i$ increases the odds of outcome vs no outcome, while\n",
    "- if $e^{\\beta_i}<1$, then predictor $X_i$ decreases the odds of outcome vs no outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7dde29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - predicting the probabilities\n",
    "probabilities = binomial_linear_model.predict()\n",
    "probabilities[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77181833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - predicting binary labels, taking \\sigma = 0.5\n",
    "predictions = (probabilities > .5).astype('int')\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ca2a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - observed vs. predicted labels\n",
    "\n",
    "predictions_df = pd.DataFrame()\n",
    "\n",
    "predictions_df['observation'] = model_frame['Churn']\n",
    "predictions_df['prediction'] = predictions\n",
    "\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb862cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - accuracy of the model\n",
    "accuracy = predictions_df['observation'] == predictions_df['prediction']\n",
    "accuracy = np.sum(accuracy)/len(accuracy)\n",
    "np.round(accuracy, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d328d054",
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Model diagnostics\n",
    "\n",
    "# - Log-likelihood of model\n",
    "model_loglike = binomial_linear_model.llf\n",
    "model_loglike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d715ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - model deviance\n",
    "residuals_deviance = binomial_linear_model.resid_dev\n",
    "model_deviance = np.sum(residuals_deviance**2)\n",
    "model_deviance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ba07c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - another way to compute model deviance\n",
    "np.sum(residuals_deviance**2) == -2*model_loglike"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61aef3d8",
   "metadata": {},
   "source": [
    "### The Akaike Information Criterion (AIC)\n",
    "\n",
    "The Akaike Information Criterion (AIC) is a statistical measure used to evaluate the goodness-of-fit of a model. It is based on the principle of parsimony, which states that simpler models should be preferred over more complex ones, all else being equal.\n",
    "\n",
    "The AIC is defined as follows:\n",
    "\n",
    "$$AIC = -2\\ln(\\mathcal{L}) + 2k $$\n",
    "\n",
    "where $\\mathcal{L}$ is the model likelihood and $k$ is the number of parameters in the model.\n",
    "\n",
    "The AIC penalizes models with more parameters, by adding a penalty term $2k$ to the log-likelihood $-2\\ln(\\mathcal{L})$. This penalty term is larger for models with more parameters, and hence it discourages overfitting and encourages simpler models.\n",
    "\n",
    "The AIC can be used to compare different models and select the best one based on their AIC values. The model with the lowest AIC value is preferred, as it strikes a good balance between goodness-of-fit and simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85680c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Akaike Information Criterion (AIC)\n",
    "binomial_linear_model.aic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7576464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - another way to compute AIC\n",
    "aic = -2*model_loglike + 2*len(predictors)\n",
    "aic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "031d0cba",
   "metadata": {},
   "source": [
    "Model effect: comparison to the Null Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d096a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Log-likelihood of model\n",
    "model_loglike = binomial_linear_model.llf\n",
    "model_loglike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858e8aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value of the constant-only loglikelihood\n",
    "null_loglike = binomial_linear_model.llnull\n",
    "null_loglike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846e8c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Comparison to the Null Model which follows the Chi-Square distribution\n",
    "\n",
    "# - differece between deviances of the Null Model and our model:\n",
    "# - Likelihood ratio chi-squared statistic; -2*(llnull - llf)\n",
    "dev_diff = binomial_linear_model.llr\n",
    "dev_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867c007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "-2*(null_loglike - model_loglike)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451864ad",
   "metadata": {},
   "source": [
    "### Target: Predict churn from all the predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe21629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - exponential of the parameters and AIC of the model using only numerical predictors (a reminder)\n",
    "np.exp(binomial_linear_model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186ed151",
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial_linear_model.aic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff71d298",
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Prepering the dataset\n",
    "\n",
    "# - droping the 'customerID' column\n",
    "model_frame = churn_data.drop(columns='customerID')\n",
    "model_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097cc58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_frame.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5c1213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - encoding 'Churn' column to binary values\n",
    "model_frame['Churn'] = model_frame['Churn'].apply(lambda x: int(x == 'Yes'))\n",
    "model_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba4ec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = model_frame.columns.drop('Churn')\n",
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9622a74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Composing the fomula of the model\n",
    "\n",
    "# - right side of the formula\n",
    "formula = ' + '.join(predictors)\n",
    "\n",
    "# - left side of the formula\n",
    "formula = 'Churn ~ ' + formula\n",
    "\n",
    "formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8620bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - fitting BLR model to the data\n",
    "binomial_linear_model = smf.logit(formula=formula, data=model_frame).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1677bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial_linear_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e4f1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - exponentials of the new model parameters\n",
    "np.exp(binomial_linear_model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb739aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - AIC of the new model\n",
    "binomial_linear_model.aic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fd0adf",
   "metadata": {},
   "source": [
    "## 2. BLR using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed7dee9",
   "metadata": {},
   "source": [
    "### Target: Predicting churn from numerical predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381f5430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - import scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2f1d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Preparing the variables \n",
    "\n",
    "# - feature matrix\n",
    "X = churn_data[['tenure', 'MonthlyCharges', 'TotalCharges']].values\n",
    "\n",
    "# - target variable\n",
    "y = churn_data['Churn'].apply(lambda x: int(x == 'Yes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a26bbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## --- Fitting the logistic model to the numerical data\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de490d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - coefficients of the model\n",
    "log_reg.coef_, log_reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5c8d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - exponentials of the model coefficients\n",
    "np.exp(log_reg.coef_), np.exp(log_reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703989fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - model's accuracy\n",
    "round(log_reg.score(X, y), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf199264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - confusion matrix for the given data\n",
    "y_pred = log_reg.predict(X)\n",
    "confusion_matrix(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a786242",
   "metadata": {},
   "source": [
    "### Target: Predicting churn from all the predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1037d7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ccb8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Composing the feature matrix\n",
    "\n",
    "# - dropping all the non-numerical and non-binary categorical columns\n",
    "X0 = churn_data.drop(columns=['customerID', 'Contract', 'PaymentMethod', 'Churn'])\n",
    "\n",
    "# - encoding binary categorical features to binary values\n",
    "X0['PaperlessBilling'] = X0['PaperlessBilling'].apply(lambda x: int(x == 'Yes'))\n",
    "X0['PhoneService'] = X0['PhoneService'].apply(lambda x: int(x == 'Yes'))\n",
    "\n",
    "X0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7300a460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - casting the data frame into a matrix\n",
    "X0 = X0.values\n",
    "X0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287a9e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - categories of the 'Contract' variable\n",
    "churn_data['Contract'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda8dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - categories of the 'PaymentMethod' variable\n",
    "churn_data['PaymentMethod'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cc4986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - we want to recreate the previous statsmodels model that was using all the predictors\n",
    "# - to achieve this we one-hot (dummy) encode non-binary categorical predictors\n",
    "# - statsmodels chooses the first category in order of appearance in the dataset as the reference category\n",
    "# - we pass the reference category manually as an argument to the OneHotEncoder\n",
    "\n",
    "enc_contract = OneHotEncoder(drop=['Month-to-month'], sparse=False)\n",
    "dummy_contract = enc_contract.fit_transform(churn_data['Contract'].values.reshape(-1, 1))\n",
    "\n",
    "enc_payment = OneHotEncoder(drop=['Bank transfer (automatic)'], sparse=False)\n",
    "dummy_payment = enc_payment.fit_transform(churn_data['PaymentMethod'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe8d740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - concatenating values of the numerical predictors and encoded binary values with the encoded non-binary values\n",
    "# - into a feature matrix\n",
    "X = np.concatenate((X0, dummy_contract, dummy_payment), axis=-1)\n",
    "display(X)\n",
    "\n",
    "# - target variable; encoding to binary values\n",
    "y = churn_data['Churn'].apply(lambda x: int(x == 'Yes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9b5b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Fitting the logistic model to all the data\n",
    "log_reg = LogisticRegression(solver='newton-cg', penalty='none')\n",
    "log_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad818962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - model's accuracy\n",
    "round(log_reg.score(X, y), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce94ff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - exponential of the model parameters\n",
    "# - ordering corresponds to the ordering of the features in the feature matrix\n",
    "np.exp(log_reg.coef_), np.exp(log_reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7777c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - confusion matrix for the given data\n",
    "y_pred = log_reg.predict(X)\n",
    "confusion_matrix(y, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c75fc9da",
   "metadata": {},
   "source": [
    "## 3. MLE for Binomial Logistic Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "691ad603",
   "metadata": {},
   "source": [
    "Say we have observed the following data: $HHTHTTHHHT$. Assume that we know the parameter $p_H$. We can compute the Likelihood function from the following equation:\n",
    "\n",
    "$\\mathcal{L}(p_H|HHTHTTHHHT)$ exactly as we did before. Now, this is the general form of the Binomial Likelihood (where $Y$ stands for the observed data):\n",
    "\n",
    "$$\\mathcal{L}(p|Y) = p_1^y(1-p_1)^{n-y}$$ \n",
    "where $y$ is the number of successes and $n$ the total number of observations. For each observed data point then we have\n",
    "\n",
    "$$\\mathcal{L}(p|y_i) = p_1^{y_i}(1-p_1)^{\\bar{y_i}}$$ \n",
    "\n",
    "where ${y_i}$ is the observed value of the outcome, $Y$, and $\\bar{y_i}$ is its complement (e.g. $1$ for $0$ and $0$ for $1$). This form just determines which value will be used in the computation of the Likelihood function at each observed data point: it will be either $p_1$ or $1-p_1$. The likelihood function for a given value of $p_1$ for the whole dataset is computed by multiplying the values of $\\mathcal{L}(p|y_i)$ across the whole dataset (remember that multiplication in Probability is what conjunction is in Logic and Algebra).\n",
    "\n",
    "**Q:** But... how do we get to $p_1$, the parameter value that we will use at each data point?\n",
    "**A:** We will search the parameter space, of course, $\\beta_0, \\beta_1, ... \\beta_k$ of linear coefficients in our Binary Logistic Model, computing $p_1$ every time, and compute the likelihood function from it! In other words: we will search the parameter space to find the combination of $\\beta_0, \\beta_1, ... \\beta_k$ that produces the *maximum of the likelihood function* similarly as we have searched the space of linear coefficients to find the combination that *minimizes the squared error* in Simple Linear Regression.\n",
    "\n",
    "So what combination of the linear coefficients is the best one?\n",
    "\n",
    "**It is the one which gives the Maximum Likelihood.** This approach, known as **Maximum Likelihood Estimation (MLE)**, stands behind *many* important statistical learning models. It presents the corner stone of the **Statistical Estimation Theory**. It is contrasted with the *Least Squares Estimation* that we have earlier used to estimate Simple and Multiple Linear Regression models.\n",
    "\n",
    "Now, there is a technical problem related to this approach. To obtain the likelihood for the whole dataset one needs to multiply as many very small numbers as there are data points. That can cause computational problems related to the smallest real numbers that can be represented by digital computers. The workaround is to use the *logarithm* of likelihood instead, known as **Log-Likelihood** ($LL$).\n",
    "\n",
    "Thus, while the Likelihood function for the whole dataset would be\n",
    "\n",
    "$$\\mathcal{L}(p|Y) = \\prod_{i=1}^{n}p_1^{y_i}(1-p_1)^{\\bar{y_i}}$$ \n",
    "the Log-Likelihood function would be:\n",
    "\n",
    "$$LL(p|Y) = \\sum_{i=1}^{n} y_ilog(p_1)+\\bar{y_i}log(1-p_1)$$ \n",
    "\n",
    "And finally here is how we solve the Binomial Logistic Regression problem:\n",
    "\n",
    "- search throught the parameter space spawned by linear coefficients $\\beta_0, \\beta_1, ... \\beta_k$,\n",
    "- predict $p_1$ from the model and a particular combination of the parameters,\n",
    "- compute the value of the Likelihood function for the whole dataset,\n",
    "- find the combination that yields the maximum of the Likelihood function.\n",
    "\n",
    "Technically, in optimization we would not go exactly for the maximum of the Likelihood function, because we use $LL$ instead of $\\mathcal{L}(p|Y)$. The solution is to **minimize the negative $LL$**, sometimes written simply as $NLL$, the Negative Log-Likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab97f61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_frame = churn_data[['Churn', 'MonthlyCharges', 'TotalCharges']]\n",
    "# - encoding 'Churn' values to binary values\n",
    "model_frame['Churn'] = model_frame['Churn'].apply(lambda x: int(x == 'Yes'))\n",
    "model_frame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39fcb877",
   "metadata": {},
   "source": [
    "Implement the model predictive pass given the parameters; the folowing `blr_predict()` function is nothing else than the implementation of the following expression:\n",
    "\n",
    "$$P(Y) = p_1 = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_kX_k)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd7cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blr_predict(params, data):\n",
    "    # - grab parameters\n",
    "    beta_0 = params[0]\n",
    "    beta_1 = params[1]\n",
    "    beta_2 = params[2]\n",
    "    # - predict: model function\n",
    "    x1 = data[\"MonthlyCharges\"]\n",
    "    x2 = data[\"TotalCharges\"]\n",
    "    # - linear combination:\n",
    "    lc = beta_0 + beta_1*x1 + beta_2*x2\n",
    "    ep = np.exp(-lc)\n",
    "    p = 1/(1+ep)\n",
    "    return(p) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ef6f1ed",
   "metadata": {},
   "source": [
    "Test `blr_predict()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f441493",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params = np.array([-2.7989, .0452, -.0006])\n",
    "predictions = blr_predict(params=test_params, data=model_frame)\n",
    "predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a7aa9a2",
   "metadata": {},
   "source": [
    "Now define the Negative Log-Likelihood function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f95afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "\n",
    "def blr_nll(params, data):\n",
    "    # - predictive pass\n",
    "    p = blr_predict(params, data)\n",
    "    # - joint negative log-likelihood\n",
    "    # - across all observations\n",
    "    nll = -binom.logpmf(data[\"Churn\"], 1, p).sum()\n",
    "    return(nll)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5382c2f3",
   "metadata": {},
   "source": [
    "Test `blr_nll()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27455f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - test blr_nll()\n",
    "test_params = np.array([-2.7989, .0452, -.0006])\n",
    "blr_nll(params=test_params, data=model_frame)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76f9f73a",
   "metadata": {},
   "source": [
    "Optimize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e9ac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# - initial (random) parameter values\n",
    "init_beta_0 = np.random.uniform(low=-3, high=0, size=1)\n",
    "init_beta_1 = np.random.uniform(low=-.05, high=.05, size=1)\n",
    "init_beta_2 = np.random.uniform(low=-.001, high=.001, size=1)\n",
    "init_pars = [init_beta_0, init_beta_1, init_beta_2]\n",
    "\n",
    "# - optimize w. Nelder-Mead\n",
    "optimal_model = minimize(\n",
    "    # - fun(parameters, args)\n",
    "    fun=blr_nll,\n",
    "    args = model_frame, \n",
    "    x0 = init_pars, \n",
    "    method='Nelder-Mead',\n",
    "    options={'maxiter':1e6, \n",
    "            'maxfev':1e6,\n",
    "            'fatol':1e-6})\n",
    "\n",
    "# - optimal parameters\n",
    "for param in optimal_model.x:\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969d7d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model.fun"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfb399fe",
   "metadata": {},
   "source": [
    "Check against `statsmodels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af6cc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = model_frame.columns.drop('Churn')\n",
    "formula = ' + '.join(predictors)\n",
    "formula = 'Churn ~ ' + formula\n",
    "print(formula)\n",
    "binomial_linear_model = smf.logit(formula=formula, data=model_frame).fit()\n",
    "binomial_linear_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09dc145d",
   "metadata": {},
   "source": [
    "Plot the Negative Log-Likelihood Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b17d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - from statsmodels: beta_0 is -2.7989, beta_1 is .0452, and beta_2 is -.0006\n",
    "beta_0 = -2.7989\n",
    "beta_1_vals = np.linspace(-.05,.05,100)\n",
    "beta_2_vals = np.linspace(-.001,.001,100)\n",
    "grid = np.array([(beta_1, beta_2) for beta_1 in beta_1_vals for beta_2 in beta_2_vals])\n",
    "grid = pd.DataFrame(grid)\n",
    "grid = grid.rename(columns={0: \"beta_1\", 1: \"beta_2\"})\n",
    "nll = []\n",
    "for i in range(grid.shape[0]):\n",
    "    pars = [beta_0, grid['beta_1'][i], grid['beta_2'][i]]\n",
    "    nll.append(blr_nll(pars, model_frame))\n",
    "grid['nll'] = nll\n",
    "grid.sort_values('nll', ascending=False, inplace=True)\n",
    "grid.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e9ed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"plotly_mimetype+notebook\"\n",
    "\n",
    "# - Mesh3d: Objective Function\n",
    "fig = go.Figure(data=[go.Mesh3d(\n",
    "    x=grid['beta_1'], \n",
    "    y=grid['beta_2'], \n",
    "    z=grid['nll'], \n",
    "    color='red', \n",
    "    opacity=0.50)])\n",
    "fig.update_layout(scene = dict(\n",
    "                    xaxis_title='Beta_1',\n",
    "                    yaxis_title='Beta_2',\n",
    "                    zaxis_title='NLL'),\n",
    "                    width=700,\n",
    "                    margin=dict(r=20, b=10, l=10, t=10))\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56992fef",
   "metadata": {},
   "source": [
    "## 4. Multinomial Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff54956b",
   "metadata": {},
   "source": [
    "The Multinomial Regression model is a powerful classification tool. Consider a problem where some outcome variable can result in more than two discrete outcomes. For example, a customer visiting a webshop can end up their visit (a) buying nothing, (b) buying some Product A, or (c) Product B, or (d) Product C, etc. If we have some information about a particular customer's journey through the website (e.g. how much time did they spend on some particular pages, did they visit the webshop before or not, or any other information that customers might have chose to disclose on their sign-up...), we can use it as a set of predictors of customer behavior resulting in any of the (a), (b), (c), (d). We do that by means of a simple extension of the Binomial Logistic Regression that is used to solve for dichotomies: enters the Multinomial Regression model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ed173cf",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "First, similar to what happens in *dummy coding*, given a set of $K$ possible outcomes we choose one of them as a *baseline*. Thus all results of the Multionomial Regression will be interpreted as effects relative to that baseline outcome category, for example: for a unit increase in predictor $X_1$ what is the change in odds to switch from (a) buying nothing to (b) buying Product A. We are already familiar with this logic, right?\n",
    "\n",
    "So, consider a set of $K-1$ independent Binary Logistic Models with only one predictor $X$ where the baseline is now referred to as $K$:\n",
    "\n",
    "$$log\\frac{P(Y_i = 1)}{P(Y_i = K)} = \\beta_{1,0} + \\beta_{1,1}X$$\n",
    "\n",
    "$$log\\frac{P(Y_i = 2)}{P(Y_i = K)} = \\beta_{2,0} + \\beta_{2,1}X$$\n",
    "\n",
    "$$log\\frac{P(Y_i = K-1)}{P(Y_i = K)} = \\beta_{K-1,0} + \\beta_{K-1,1}X$$\n",
    "\n",
    "Obviously, we are introducing a set of new regression coefficients ($\\beta_{k,\\cdot}$) for each possible value of the outcome $k = 1, 2,.., K-1$. The *log-odds* are on the LHS while the linear model remains on the RHS.\n",
    "\n",
    "Now we exponentiate the equations to arrive at the expressions for *odds*:\n",
    "\n",
    "$$\\frac{P(Y_i = 1)}{P(Y_i = K)} = e^{\\beta_{1,0} + \\beta_{1,1}X}$$\n",
    "\n",
    "$$\\frac{P(Y_i = 2)}{P(Y_i = K)} = e^{\\beta_{2,0} + \\beta_{2,1}X}$$\n",
    "\n",
    "$$\\frac{P(Y_i = K-1)}{P(Y_i = K)} = e^{\\beta_{K-1,0} + \\beta_{K-1,1}X}$$\n",
    "\n",
    "And solve for $P(Y_i = 1), P(Y_i = 2),.. P(Y_i = K-1)$:\n",
    "\n",
    "$$P(Y_i = 1) = P(Y_i = K)e^{\\beta_{1,0} + \\beta_{1,1}X}$$\n",
    "\n",
    "$$P(Y_i = 2) = P(Y_i = K)e^{\\beta_{2,0} + \\beta_{2,1}X}$$\n",
    "\n",
    "$$P(Y_i = K-1) = P(Y_i = K)e^{\\beta_{K-1,0} + \\beta_{K-1,1}X}$$\n",
    "\n",
    "From the fact that all probabilities $P(Y_i = 1), P(Y_i = 2), .., P(Y_i = K)$ must sum to one it can be shown that\n",
    "\n",
    "$$P(Y_i = K) = \\frac{1}{1+\\sum_{k=1}^{K-1}e^{\\beta_{k,0} + \\beta_{k,1}X}}$$\n",
    "\n",
    "Because:\n",
    "\n",
    "(1) $P(Y_i = 1) + P(Y_i = 2) + ... + P(Y_i = K) = 1$, we have\n",
    "\n",
    "(2) $P(Y_i = K) + P(Y_i = K)e^{\\beta_{1,0} + \\beta_{1,1}X} + P(Y_i = K)e^{\\beta_{2,0} + \\beta_{2,1}X} + ... + P(Y_i = K=1)e^{\\beta_{K-1,0} + \\beta_{K-1,1}X} = 1$ \n",
    "\n",
    "(3) and then replace $e^{\\beta_{1,0} + \\beta_{1,1}X}$ by $l_1$, $e^{\\beta_{2,0} + \\beta_{2,1}X}$ by $l_2$, and $e^{\\beta_{k,0} + \\beta_{k,1}X}$ by $l_k$ in the general case, we have\n",
    "\n",
    "(4) $P(Y_i = K) + P(Y_i = K)e^{l_1} + P(Y_i = K)e^{l_2} + ... + P(Y_i = K-1)e^{l_{K-1}} = 1$\n",
    "\n",
    "(5) $P(Y_i = K)[1 + e^{l_1} + e^{l_2} + ... + e^{l_{K-1}}] = 1$ so that \n",
    "\n",
    "(6) $P(Y_i = K) = \\frac{1}{1 + e^{l_1} + e^{l_2} + ... + e^{l_{K-1}}} = \\frac{1}{1+\\sum_{k=1}^{K-1}e^{\\beta_{k,0} + \\beta_{k,1}X}}$\n",
    "\n",
    "It is easy now to derive the expressions for all $K-1$ probabilities of the outcome resulting in a particular class:\n",
    "\n",
    "$$P(Y_i = 1) = \\frac{e^{\\beta_{1,0} + \\beta_{1,1}X}}{1+\\sum_{k=1}^{K-1}e^{\\beta_{k,0} + \\beta_{k,1}X}}$$\n",
    "\n",
    "$$P(Y_i = 2) = \\frac{e^{\\beta_{2,0} + \\beta_{2,1}X}}{1+\\sum_{k=1}^{K-1}e^{\\beta_{k,0} + \\beta_{k,1}X}}$$\n",
    "\n",
    "$$P(Y_i = K-1) = \\frac{e^{\\beta_{K-1,0} + \\beta_{K-1,1}X}}{1+\\sum_{k=1}^{K-1}e^{\\beta_{k,0} + \\beta_{k,1}X}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea73198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - loading the dataset\n",
    "# - Kaggle: https://www.kaggle.com/datasets/uciml/iris\n",
    "# - place it in your _data/ directory\n",
    "iris_data = pd.read_csv(os.path.join(data_dir, 'Iris.csv'), index_col='Id')\n",
    "iris_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2253536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - counting the instances of each category\n",
    "iris_data['Species'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f9d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - info on the variables\n",
    "iris_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59557bd",
   "metadata": {},
   "source": [
    "### Target: predict species from all continuous predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ae5739",
   "metadata": {},
   "source": [
    "We use Multinomial Logistic Regression Model to predict the most probable category for the given observation $\\textbf{x}$ with features $(x_1, x_2, \\ldots, x_k)$. Assume that our target variable $y$ belongs to one of categories from the set $\\{1, 2, \\ldots, C\\}$. In MNR we usually select one category as the referrence category; let that be the category $C$. Then, the probability that the target variable $y$ belongs to category $c = 1,\\ldots,C-1$ is calculated via\n",
    "\n",
    "$$P(y = c) = \\frac{e^{\\beta^{(c)}_1x_1 + \\beta^{(c)}_2x_2 + \\cdots + \\beta^{(c)}_kx_k + \\beta_0}}{1+\\sum_{j=1}^{C-1}e^{\\beta^{(j)}_1x_1 + \\beta^{(j)}_2x_2 + \\cdots + \\beta^{(j)}_kx_k + \\beta_0}},$$\n",
    "\n",
    "and the probability that it belogns to the referrence category $C$ is \n",
    "\n",
    "$$P(y = C) = \\frac{1}{1+\\sum_{j=1}^{C-1}e^{\\beta^{(j)}_1x_1 + \\beta^{(j)}_2x_2 + \\cdots + \\beta^{(j)}_kx_k + \\beta_0}},$$\n",
    "\n",
    "where $\\beta^{(j)}_1, \\beta^{(j)}_2, \\ldots, \\beta^{(j)}_k,\\ j=1,\\ldots,C$ are the model's parameters for predictors and target variable categories, and $n$ is the intercept of the model.\n",
    "\n",
    "After calculating all the probabilities $P(y = c),\\ c=1,\\ldots,C$ we predict the target variable as\n",
    "\n",
    "$$\\hat{y} = \\textrm{argmax}_{c=1,\\ldots,C}P(y=c).$$\n",
    "\n",
    "The model is estimated by MLE (Maximum Likelihood Estimation). For each category $c$ - except for the referrence $C$, of course - we obtain a set of coefficients. Each model coefficient, in each category, tells us about the $\\Delta_{odds}$ in favor of the target category, for a unit change of a predictor, in comparison with the baseline category, and given that everything else is kept constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fc776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Preparing the variables \n",
    "\n",
    "# - feature matrix\n",
    "X = iris_data.drop(columns='Species')\n",
    "# - we append a constant column of ones to the feature matrix\n",
    "X = sm.add_constant(X)\n",
    "print(X[:10])\n",
    "\n",
    "\n",
    "# - we impose the ordering to the categories of the target vector; the first category is the referrence category\n",
    "cat_type = pd.CategoricalDtype(categories=[\"Iris-versicolor\", \"Iris-virginica\", \"Iris-setosa\"], ordered=True)\n",
    "y = iris_data['Species'].astype(cat_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de73ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - fitting the MNR model to the data; we use the Newton's Conjugate Gradient method as the optimizer to compute the\n",
    "# - models coefficients\n",
    "mnr_model = sm.MNLogit(exog=X, endog=y).fit(method='ncg', maxiter=150)\n",
    "mnr_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ed02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - confusion matrix for our model and given data; rows/columns are on par with the ordering of categorical variable\n",
    "mnr_model.pred_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c912d6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - accuracy of the model\n",
    "correct_classes = np.trace(mnr_model.pred_table())\n",
    "print(f'Correct observations: {correct_classes}')\n",
    "num_obs = np.sum(mnr_model.pred_table())\n",
    "print(f'Total observations: {num_obs}')\n",
    "print(f'The accuracy of our model: {round(correct_classes/num_obs, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c85668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - model's prediction of probabilities; columns correspond to the ordering of categorical variable\n",
    "mnr_model.predict()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d63497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - model's prediction of categories; numbers correspond to the ordering of categorical variable\n",
    "preds = np.argmax(mnr_model.predict(), axis=-1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4b6bd4",
   "metadata": {},
   "source": [
    "### Multicolinearity in Multinomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8593ef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Step 1: recode categorical target variable to integer values, \n",
    "# - just in order to be able to run a multiple linear regression on the data:\n",
    "y_code = y.cat.codes\n",
    "y_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edf57fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Step 2: produce a Multiple Linear Regression model for the data \n",
    "mnr_model = sm.OLS(exog=X, endog=y_code).fit()\n",
    "mnr_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a6c2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: compute VIFs for the predictors\n",
    "predictors = iris_data.columns.drop('Species')\n",
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6f7e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - appending the columns of ones to the predictors' data\n",
    "model_frame_predictors = sm.add_constant(iris_data[predictors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c4a1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - computing VIFs\n",
    "vifs = [variance_inflation_factor(model_frame_predictors.values, i) for i in range(1, len(predictors)+1)]\n",
    "vifs = np.array(vifs).reshape(1, -1)\n",
    "pd.DataFrame(vifs, columns=predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e66dcd",
   "metadata": {},
   "source": [
    "### Multinomial Logistic Regression using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439a3b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - import scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89165ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Preparing the variables \n",
    "\n",
    "# - feature matrix\n",
    "X = iris_data.drop(columns='Species').values\n",
    "\n",
    "# - target variable\n",
    "y = iris_data['Species']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42464e77",
   "metadata": {},
   "source": [
    "**N.B.** scikit-learn does not implement the referrence category automatically! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178f72bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Fitting the logistic model to the numerical data\n",
    "# - scikit-learn does not implement the referrence category automatically \n",
    "log_reg = LogisticRegression(solver='newton-cg', penalty='none')\n",
    "log_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be935c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - coefficents of the model; rows correspond to the order of appearance of categories in the target variable\n",
    "log_reg.coef_, log_reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dead08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - model's accuracy\n",
    "round(log_reg.score(X, y), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5416aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - predictions\n",
    "y_pred = log_reg.predict(X)\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114fcbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - confusion matrix for the given data\n",
    "# - rows/columns rows correspond to the order of appearance of categories in the target variable\n",
    "confusion_matrix(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b940d96c",
   "metadata": {},
   "source": [
    "### Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad196fb",
   "metadata": {},
   "source": [
    "- [Logistic Regression — Gradient Descent Optimization — Part 1](https://medium.com/technology-nineleaps/logistic-regression-gradient-descent-optimization-part-1-ed320325a67e)\n",
    "- [Logistic regression with gradient descent —Tutorial Part 1 — Theory](https://medium.com/@edwinvarghese4442/logistic-regression-with-gradient-descent-tutorial-part-1-theory-529c93866001)\n",
    "- [Logistic regression with gradient descent — Tutorial Part 2— CODE](https://medium.com/@edwinvarghese4442/logistic-regression-with-gradient-descent-tutorial-part-2-code-a4544bb1505)\n",
    "- [LINEAR SUPERVISED LEARNING SERIES: Part 2: Logistic regression](https://rezaborhani.github.io/mlr/blog_posts/Linear_Supervised_Learning/Part_2_logistic_regression.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4071b62b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012d9eaa",
   "metadata": {},
   "source": [
    "DataKolektiv, 2022/23.\n",
    "\n",
    "[hello@datakolektiv.com](mailto:goran.milovanovic@datakolektiv.com)\n",
    "\n",
    "![](../img/DK_Logo_100.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec890e1",
   "metadata": {},
   "source": [
    "<font size=1>License: [GPLv3](https://www.gnu.org/licenses/gpl-3.0.txt) This Notebook is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This Notebook is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this Notebook. If not, see http://www.gnu.org/licenses/.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
