{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67b65bdc",
   "metadata": {},
   "source": [
    "# DATA SCIENCE SESSIONS VOL. 3\n",
    "### A Foundational Python Data Science Course\n",
    "## Tasklist 13 and 14: Simple Linear Regression. Multiple Linear Regression.\n",
    "\n",
    "[&larr; Back to course webpage](https://datakolektiv.com/)\n",
    "\n",
    "Feedback should be send to [goran.milovanovic@datakolektiv.com](mailto:goran.milovanovic@datakolektiv.com). \n",
    "\n",
    "These notebooks accompany the DATA SCIENCE SESSIONS VOL. 3 :: A Foundational Python Data Science Course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4dbdf7",
   "metadata": {},
   "source": [
    "![](../img/IntroRDataScience_NonTech-1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e97431",
   "metadata": {},
   "source": [
    "### Lecturers\n",
    "\n",
    "[Goran S. Milovanović, PhD, DataKolektiv, Chief Scientist & Owner](https://www.linkedin.com/in/gmilovanovic/)\n",
    "\n",
    "[Aleksandar Cvetković, PhD, DataKolektiv, Consultant](https://www.linkedin.com/in/alegzndr/)\n",
    "\n",
    "[Ilija Lazarević, MA, DataKolektiv, Consultant](https://www.linkedin.com/in/ilijalazarevic/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9da7dd",
   "metadata": {},
   "source": [
    "![](../img/DK_Logo_100.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52c2322",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc6f6683",
   "metadata": {},
   "source": [
    "### Intro \n",
    "\n",
    "The goal of this Tasklist is to consolidate our knowledge of theoretical and practical insights provided in sessions 13 and 14 on (Multiple) Linear Regression. So far we have gone through simple and multiple linear regressions, parametric bootstrap and part and partial correlation. Looking back at the things that we have learned in the previous sessions we are now beginning to develop a full circle of neccessary steps in one basic data science project. \n",
    "\n",
    "In this tasklist we are going to use [Car Price Prediction](https://www.kaggle.com/datasets/hellbuoy/car-price-prediction) data set from Kaggle. Read about the problem statement and the goal of the study from the link provided. **The only thing that needs to happen with the Car Price Prediction dataset here is for you to study and understand the Linear Regression Modeling of this data set as it is provided in this notebook.**\n",
    "\n",
    "**YOUR REAL TASK is to analyze something else:** the **Market Mix data** provided as `market_mix.csv` in the `_data` directory of this session (the source file is found in the [veer064/Linear-Regression](https://github.com/veer064/Linear-Regression) GitHub repo). **The only thing that you need to do about the Market Mix data set is to use Multiple Linear Regression to predict the `NewVolSales` variable and inspect the multicolinearity present in the model.** So, fist study the analysis of the Car Price Prediction as it is provided here, and then simply load `market_mix.csv`, clean all cell contents, and repeat everything that is done with Multiple Linear Regression in the Car Price Prediction study on the Market Mix data set where your outcome variable will be `NewVolSales`.\n",
    "\n",
    "Data is provided in two files in `_data` folder. Namely:\n",
    "- `car_prices.csv` - file with data we are going to work with,\n",
    "- `data_dictionary.xlsx` - file that contains descriptions for each column in CSV file. **Warning** `carCompany` column from the dictionary is named `CarName` in CSV file.\n",
    "\n",
    "We will use words column, feature, predictor interchangably though the tasklist, so do not let that confuse you.\n",
    "\n",
    "**We strongly suggest studying Sessions 13 and 14 thoroughly and the two texts provided in the References section of this TaskList before proceeding.**\n",
    "\n",
    "Also, idea is to use Python libraries we have introduced in Sessions 13 and 14.\n",
    "\n",
    "\n",
    "Let's start by importing neccessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef5bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2a4445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from scipy import stats \n",
    "\n",
    "from statsmodels.regression.linear_model import RegressionResultsWrapper\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6523ac68",
   "metadata": {},
   "source": [
    "Like we said, we have the data set in `_data` folder. Let's load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613bb18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "work_dir = os.getcwd()\n",
    "data_dir = os.path.join(work_dir, \"_data\")\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92091576",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(data_dir, 'car_prices.csv'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0591cbc3",
   "metadata": {},
   "source": [
    "Before we go any further, we will define helper methods for easier preview of model performance analysis. It is not necessary to understand the code for the following two methods at hand in order to proceed with the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b35d297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a method that will return metrics for plotting influential points\n",
    "def get_influential_points_summary(model: RegressionResultsWrapper, k: int = 1):\n",
    "    model_inf = model.get_influence()\n",
    "    # inf_frame = model_inf.summary_frame()\n",
    "    # w_cookD = np.argwhere(model_inf.cooks_distance[0] > 1)\n",
    "    n = len(model.resid)\n",
    "    # w_leverage = np.argwhere(model_inf.hat_matrix_diag > 2*(k+1)/n)\n",
    "\n",
    "    inf_plot_frame = pd.DataFrame(columns=['Residuals', 'Leverage', 'Cook Dist.'])\n",
    "\n",
    "    inf_plot_frame['Residuals'] = model.resid\n",
    "    inf_plot_frame['Leverage'] = model_inf.hat_matrix_diag\n",
    "    inf_plot_frame['Cook Dist.'] = model_inf.cooks_distance[0]\n",
    "\n",
    "    return inf_plot_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02630d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define method to plot model performance analyis\n",
    "def plot_model_analysis(model: RegressionResultsWrapper, target) -> None:\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 8))\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=.4)\n",
    "    \n",
    "    axes = axes.flatten()\n",
    "\n",
    "    sns.histplot(model.resid, element='step', linewidth=.0, ax=axes[0]);\n",
    "    sns.despine()\n",
    "    # axes[0].grid(alpha=.3)\n",
    "    axes[0].set_title('Residuals hisogram', fontsize=10)\n",
    "    \n",
    "\n",
    "    sm.qqplot(model.resid, line='q', ax=axes[1])\n",
    "    sns.despine()\n",
    "    axes[1].set_title('Q-Q Plot of model residuals', fontsize=10)\n",
    "    \n",
    "    ip_summary = get_influential_points_summary(model)\n",
    "    \n",
    "    sns.scatterplot(data=ip_summary, x='Leverage', y='Residuals', size='Cook Dist.', hue=(ip_summary['Cook Dist.'] > 1), legend=None, ax=axes[2])\n",
    "    sns.despine()\n",
    "    axes[2].set_title(\"Influence Plot\\n Size of the blob corresponds to Cook's distance\", fontsize=10);\n",
    "\n",
    "    sns.scatterplot(x=target, y=linear_model.resid, s=10, ax=axes[3])\n",
    "    sns.despine()\n",
    "    axes[3].axhline(y=0, color='red')\n",
    "    axes[3].grid(alpha=.4)\n",
    "    axes[3].set_title('Price vs Residual', fontsize=10)\n",
    "    axes[3].set_xlabel('price')\n",
    "    axes[3].set_ylabel('residuals')\n",
    "\n",
    "    sns.scatterplot(x=target, y=linear_model.predict(), s=10, ax=axes[4])\n",
    "    sns.despine()\n",
    "    axes[4].axline((0, 0), (1, 1), color='red')\n",
    "    axes[4].grid(alpha=.4)\n",
    "    axes[4].set_title('Price vs Predicted', fontsize=10)\n",
    "    axes[4].set_xlabel('price')\n",
    "    axes[4].set_ylabel('predicted')\n",
    "\n",
    "    sns.scatterplot(x=target, y=linear_model.resid/target, s=10, ax=axes[5])\n",
    "    sns.despine()\n",
    "    axes[5].axhline(y=0, color='red')\n",
    "    axes[5].grid(alpha=.4)\n",
    "    axes[5].set_title('Price vs residual/price', fontsize=10)\n",
    "    axes[5].set_xlabel('price')\n",
    "    axes[5].set_ylabel('residual/price')\n",
    "\n",
    "    fig.suptitle('Model analysis', fontsize=15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60966200",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af1584f6",
   "metadata": {},
   "source": [
    "**01.** Before we go into modeling, we want to make sure we understand the data at hand. Remember EDA? Great! Let's do it here.\n",
    "\n",
    "a) Give a short preview of the data, number of missing values per column, information about the data types, and descriptive statistics for data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc753c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19529867",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6bc80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6944b6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864c1762",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b301a7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.select_dtypes(include='object').columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcd3327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how much observations do we have per each category for each categorical variable\n",
    "for column in df.select_dtypes(include='object').columns[1:]:\n",
    "    print(df[column].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7a6f899",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "- 25 predictor variables, and 1 outcome variable (price).\n",
    "- 15 predictor variables are of numerical type. One of these is ordinal variable (*symboling*).\n",
    "- 10 predictor variables are of categorical type.\n",
    "- There are no missing values in data set.\n",
    "- *carName* categorical variable has the highest cardinaly among all variables of same type.\n",
    "- Some categories have very few observations e.g. *drivewheel* has only 3 observations for *rear* value.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d9c3a09",
   "metadata": {},
   "source": [
    "b) `price` is obviously the variable that we will be predicting based on other predictor variables. Create charts for each of numerical variable depicting how its affects the price.\n",
    "\n",
    "There are multiple ways, but try to come up the elegant one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb9f9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_variables = df.select_dtypes(exclude='object').columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397e7176",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numerical_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ad17c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = df[numerical_variables].melt(id_vars='price', value_vars=numerical_variables[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d892cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(data=_df, col='variable', col_wrap=5, sharex=False, height=2, aspect=1.3)\n",
    "g.map(sns.scatterplot, 'value', 'price', s=10);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a9fe814",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bc045a3",
   "metadata": {},
   "source": [
    "c) Do the same thing for categorical variables. You can exclude `CarName` from overview. Also, have in mind you have more than one way at your disposal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708bf6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_variables = df.select_dtypes(include='object').columns.to_list() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e58dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = df[categorical_variables + ['price']].melt(id_vars='price', value_vars=categorical_variables[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d5e24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(data=_df, col='variable', col_wrap=5, sharey=False, height=2, aspect=1.3)\n",
    "g.map(sns.boxplot, 'price', 'value' );"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8e6d817",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31f60ed6",
   "metadata": {},
   "source": [
    "d) Figure out the way to get Peason's correlation coefficient between each of the numerical variables. Remember, it has its statistic value and p-value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c3b024",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numerical_variables].corr(method='pearson')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b2fb3e1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c210a2f6",
   "metadata": {},
   "source": [
    "e) Plot these coefficients using `heatmap` chart from `seaborn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047af0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(df[numerical_variables].corr(method='pearson'), annot=True, fmt='.2f');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32fc9c0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85d8e8fc",
   "metadata": {},
   "source": [
    "f) Let's define our significancy level $\\alpha$ at 0.05. Use `heatmap` to plot which feature pairs have their Pearsons' correlation significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84b6b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for i1 in numerical_variables[1:]:\n",
    "    for i2 in numerical_variables[1:]:\n",
    "        data.append({'name_a': i1, 'name_b': i2, 'pvalue': stats.pearsonr(df[i1], df[i2]).pvalue})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c0a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = pd.DataFrame(data).pivot(index='name_a', columns='name_b', values='pvalue') < 0.05\n",
    "_df = _df[numerical_variables[1:]]\n",
    "_df = _df.T[numerical_variables[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ec99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "sns.heatmap(_df);\n",
    "plt.xlabel('')\n",
    "plt.ylabel('');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94bb4a0b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8ad71c5",
   "metadata": {},
   "source": [
    "2. Since there is a lot of both numerical and categorical variables, let's start with simple linear regression."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7671a9cb",
   "metadata": {},
   "source": [
    "a) Regress `price` on categorical variable `fueltype` and describe OLS regression results. Then analyse residuals and influential observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f4d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - fitting the linear model to the data\n",
    "linear_model = smf.ols(formula='price ~ fueltype', data=df).fit()\n",
    "print(linear_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aecad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_analysis(linear_model, df['price'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69bc4d1f",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "Short version: Our model is not statistically significant. \n",
    "\n",
    "Long version: \n",
    "- Null hypothesis (H0) says β1 = β2 = ... = 0 (regression model does not exist). \n",
    "- Alternative hypothesis H1 says there is at least one regression coefficient that is different from 0.\n",
    "- By looking at the F-statistic and it's probability (p-value) we should either have a proof for rejecting H0 or not. In our case we do not have evidence to be able to reject H0 and we say this regression model is not good i.e. there is no relationshipt between predictor variable and target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4402aec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b88f56f9",
   "metadata": {},
   "source": [
    "b) Regress `price` on categorical variable `carbody` and describe the results. Use category *hatcback* as reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5ab83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - fitting the linear model to the data\n",
    "linear_model = smf.ols(formula='price ~ C(carbody, Treatment(reference=\"hatchback\"))', data=df).fit()\n",
    "print(linear_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc674080",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_analysis(linear_model, df['price'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6cb019d7",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "This model's F-statistic probability (p-value) is way below 0.05 alpha value. Based on this we have the evidence to reject H0 and say that at least one of the model's coefficients has it's value different from 0. Therefore, we continue with interpreting the rest of the regression report.\n",
    "\n",
    "Model has very low value of R2. It's not that good at describing the variance in price based on `carbody` variable.\n",
    "\n",
    "Based on p-values and stdandard errors of coefficients, we can see that most of the coefficients are statistically significant (except for coefficient for *wagon* category)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4f8a2fc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3829b90c",
   "metadata": {},
   "source": [
    "c) Let's try multiple linear regression model now. First use only numerical features when modeling and describe OLS regression results and results of visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30aeb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec39557",
   "metadata": {},
   "outputs": [],
   "source": [
    "fomula_features = ' + '.join(numerical_variables[1:-1])\n",
    "fomula_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d7f9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - fitting the linear model to the data\n",
    "linear_model = smf.ols(formula='price ~ ' + fomula_features, data=df).fit()\n",
    "linear_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3730f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_analysis(linear_model, target=df['price'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b796d1a",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "This model's F-statistic probability (p-value) is way below 0.05 alpha value. Based on this we can reject null hypothesis and say that there is at least one model's coefficient that is different from 0.\n",
    "\n",
    "Based on R2 value of 0.852 we can say this model is pretty good at describing variance in price by numerical predictors.\n",
    "\n",
    "Most model's coefficients have statistically significant effect on describing price.\n",
    "\n",
    "Residual's histogram and QQ plot show that residuals have distribution very similar to the normal distribution.\n",
    "\n",
    "Predicted vs residual shows something that looks like a heteroskedastisity. This can bee seen on Price vs. Predicted plot as well.\n",
    "\n",
    "However, the last plot shows ratio between residual/price and price, and this gives insight in percentage of errors for each prediction. Here we see that most of the prediction errors are in +-25% range."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14db6e96",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e086cd76",
   "metadata": {},
   "source": [
    "d) Get the VIF for numerical features and explain the findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ba0ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Variance Inflation Factors (VIFs)\n",
    "\n",
    "# - appending the columns of ones to the predictors' data\n",
    "model_frame_predictors = sm.add_constant(df[numerical_variables[1:-1]])\n",
    "model_frame_predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d02bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The lower bound of VIF is 1; \n",
    "# - there is no upper bound;\n",
    "# - VIF > 2 indicates high variance inflation\n",
    "vifs = [variance_inflation_factor(model_frame_predictors.values, i) for i in range(1, len(numerical_variables[1:-1])+1)]\n",
    "vifs = np.array(vifs).reshape(1, -1)\n",
    "\n",
    "df_vifs = pd.DataFrame(vifs, columns=numerical_variables[1:-1]).T\n",
    "df_vifs.rename(columns={0: 'vif'}).sort_values(by='vif', ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "823ba791",
   "metadata": {},
   "source": [
    "Conclusion: If we say that VIF > 2 is a sign of variance inflation, we can see that most of our predictors have their variance inflated due to collinearity with other predictors. One interesting thing here are top 2 predictors (`citympg` and `highwaympg`), that have high VIF. This matches with findinds from the heatmap of correlations. We saw previously that Pearson's correlation coefficient between these two is 0.97!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e9d1d8b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d4eb85f",
   "metadata": {},
   "source": [
    "e) Let's try multiple linear regression model, but now with only categorical features when modeling and describe OLS regression results and results of visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b09e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462cf018",
   "metadata": {},
   "outputs": [],
   "source": [
    "fomula_features = ' + '.join(categorical_variables[1:])\n",
    "fomula_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c6a92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - fitting the linear model to the data\n",
    "linear_model = smf.ols(formula='price ~ ' + fomula_features, data=df).fit()\n",
    "linear_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1fa937",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_analysis(linear_model, target=df['price'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37e0e8de",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "This model's F-statistic probability (p-value) is way below 0.05 alpha value. Based on this we can reject null hypothesis and say that there is at least one model's coefficient that is different from 0.\n",
    "\n",
    "Based on R2 value of 0.852 we can say this model is pretty good at describing variance in price by numerical predictors.\n",
    "\n",
    "Most model's coefficients have statistically significant effect on describing price.\n",
    "\n",
    "Residual's histogram and QQ plot show that residuals have distribution very similar to the normal distribution.\n",
    "\n",
    "Predicted vs residual shows something that looks like a heteroskedastisity. This can bee seen on Price vs. Predicted plot as well.\n",
    "\n",
    "However, the last plot shows ratio between residual/price and price, and this gives insight in percentage of errors for each prediction. Here we see that most of the prediction errors are in +-40% range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2196ca36",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af93edd2",
   "metadata": {},
   "source": [
    "f) Do the regression analysis for the model that encompasses all of the predictors. Describe the results and make conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533effe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors_list = numerical_variables[1:-1] + categorical_variables[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ef8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fomula_features = ' + '.join(predictors_list)\n",
    "fomula_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f63b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - fitting the linear model to the data\n",
    "linear_model = smf.ols(formula='price ~ ' + fomula_features, data=df).fit()\n",
    "linear_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b16b685",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_analysis(linear_model, target=df['price'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ee53320",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "This model's F-statistic probability (p-value) is way below 0.05 alpha value. Based on this we can reject null hypothesis and say that there is at least one model's coefficient that is different from 0.\n",
    "\n",
    "Based on R2 value of 0.94 we can say this model is even better than previous models at describing variance in price by predictors.\n",
    "\n",
    "There is a lot of model's coefficients that have statistically significant effect on describing price.\n",
    "\n",
    "Residual's histogram and QQ plot show that residuals have distribution very similar to the normal distribution.\n",
    "\n",
    "Predicted vs residual shows something that looks like a heteroskedastisity. This can bee seen on Price vs. Predicted plot as well.\n",
    "\n",
    "However, the last plot shows ratio between residual/price and price, and this gives insight in percentage of errors for each prediction. Here we see that most of the prediction errors are in +-20% range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590d0bbb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f9ca484",
   "metadata": {},
   "source": [
    "3. Use `sklearn` to fit the linear regression model, get the predictions, plot the Price vs. Predicted chart and calculate R2 metric for the prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d358b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numerical_variables[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e6c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([df[numerical_variables[1:-1]], pd.get_dummies(df[categorical_variables[1:]])], axis=1)\n",
    "y = df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d2f447",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bb569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = lr.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa052c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d5ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=y, y=y_preds)\n",
    "sns.despine()\n",
    "plt.grid(alpha=.3)\n",
    "plt.axline((0, 0), (1, 1), color='red')\n",
    "plt.title('Price vs Predicted plot', fontsize=15)\n",
    "plt.xlabel('price')\n",
    "plt.ylabel('predicted');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7a824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importances = pd.DataFrame(list(zip(lr.coef_, lr.feature_names_in_)))\n",
    "df_importances = df_importances.rename(columns={0: 'coefficient', 1: 'feature'})\n",
    "df_importances = df_importances.sort_values(by='coefficient', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7364313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 10))\n",
    "df_importances.set_index('feature').plot(kind='barh', ax=plt.gca())\n",
    "sns.despine()\n",
    "plt.grid(alpha=.3);\n",
    "plt.title('Feature importances', fontsize=15);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52b9dd30",
   "metadata": {},
   "source": [
    "For each dummy encoded categorical variable, one of its values is taken for the base line. You have to be sure what category is base line so you can interpret these feature importances correctly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79d15e0f",
   "metadata": {},
   "source": [
    "### References\n",
    "- [Interpreting Linear Regression Through statsmodels.summary()](https://medium.com/swlh/interpreting-linear-regression-through-statsmodels-summary-4796d359035a)\n",
    "- [Mastering f-statistics in Linear Regression: Formula, Examples](https://vitalflux.com/interpreting-f-statistics-in-linear-regression-formula-examples/amp/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5dc0348",
   "metadata": {},
   "source": [
    "DataKolektiv, 2022/23.\n",
    "\n",
    "[hello@datakolektiv.com](mailto:goran.milovanovic@datakolektiv.com)\n",
    "\n",
    "![](../img/DK_Logo_100.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8e160a",
   "metadata": {},
   "source": [
    "<font size=1>License: [GPLv3](https://www.gnu.org/licenses/gpl-3.0.txt) This Notebook is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This Notebook is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this Notebook. If not, see http://www.gnu.org/licenses/.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
